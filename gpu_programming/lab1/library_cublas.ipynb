{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jNcUmN8yNY2"
      },
      "source": [
        "# Vérifie que le compilateur nvcc de GPU est bien installé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wf4hmeeuj7Q",
        "outputId": "85fb84a9-3f58-4820-e89a-f91f200a572d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7i6-i1lyBb3"
      },
      "source": [
        "# Vérifie qu'on a bien activé le GPU de notre machine (fictive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrMkkUNKu9HJ",
        "outputId": "75a69801-46ab-45cb-d024-97b864d5d3c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de GPUs Dispoc:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Nombre de GPUs Dispoc: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_4hh5-UyV39"
      },
      "source": [
        "# Installer CUDA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhJApZqmvDQn",
        "outputId": "72ad7886-44a1-4b38-fafa-86ee529f12eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libcudnn8\n",
            "0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 444 MB of archives.\n",
            "After this operation, 1,099 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcudnn8 8.9.7.29-1+cuda12.2 [444 MB]\n",
            "Fetched 444 MB in 6s (78.2 MB/s)\n",
            "Selecting previously unselected package libcudnn8.\n",
            "(Reading database ... 124926 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.9.7.29-1+cuda12.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.9.7.29-1+cuda12.2) ...\n",
            "Setting up libcudnn8 (8.9.7.29-1+cuda12.2) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y libcudnn8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5szvZOybye88"
      },
      "source": [
        "# Nous allons nous servir de ce code pour apprécier cuBLAS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhWl9XkfvweT",
        "outputId": "ed6fc189-5957-4674-87bb-c14a7a161d73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing large_matrix_mul.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile large_matrix_mul.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cublas_v2.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define N 1024  // Taille des matrices\n",
        "\n",
        "// Fonction pour initialiser la matrice avec des valeurs aléatoires\n",
        "void initialize_matrix(float *matrix, int size) {\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        matrix[i] = (float)(rand() % 100);  // Valeurs entre 0 et 99\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    float *A, *B, *C;\n",
        "    float *d_A, *d_B, *d_C;\n",
        "\n",
        "    // Allocation de mémoire pour les matrices sur le CPU\n",
        "    A = (float*)malloc(N * N * sizeof(float));\n",
        "    B = (float*)malloc(N * N * sizeof(float));\n",
        "    C = (float*)malloc(N * N * sizeof(float));\n",
        "\n",
        "    // Initialisation des matrices A et B avec des valeurs aléatoires\n",
        "    srand(time(NULL));  // Initialisation de la graine pour rand()\n",
        "    initialize_matrix(A, N * N);\n",
        "    initialize_matrix(B, N * N);\n",
        "\n",
        "    // Création du handle cuBLAS\n",
        "    cublasHandle_t handle;\n",
        "    cublasCreate(&handle);\n",
        "\n",
        "    // Allocation de mémoire sur le GPU\n",
        "    cudaMalloc((void**)&d_A, sizeof(float) * N * N);\n",
        "    cudaMalloc((void**)&d_B, sizeof(float) * N * N);\n",
        "    cudaMalloc((void**)&d_C, sizeof(float) * N * N);\n",
        "\n",
        "    // Copie des matrices A et B vers le GPU\n",
        "    cudaMemcpy(d_A, A, sizeof(float) * N * N, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, B, sizeof(float) * N * N, cudaMemcpyHostToDevice);\n",
        "\n",
        "    const float alpha = 1.0f, beta = 0.0f;\n",
        "\n",
        "    // Multiplication de matrices : C = alpha * A * B + beta * C\n",
        "    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,\n",
        "                N, N, N,\n",
        "                &alpha, d_A, N, d_B, N,\n",
        "                &beta, d_C, N);\n",
        "\n",
        "    // Copie du résultat de C vers le CPU\n",
        "    cudaMemcpy(C, d_C, sizeof(float) * N * N, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Affichage partiel (pour ne pas saturer la sortie)\n",
        "    printf(\"Quelques éléments de la matrice résultante C:\\n\");\n",
        "    for (int i = 0; i < 5; i++) {\n",
        "        for (int j = 0; j < 5; j++) {\n",
        "            printf(\"%8.2f \", C[i * N + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Libération de la mémoire GPU et CPU\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    free(A);\n",
        "    free(B);\n",
        "    free(C);\n",
        "    cublasDestroy(handle);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJMZSGHMvxx3"
      },
      "outputs": [],
      "source": [
        "!nvcc -o large_matrix_mul large_matrix_mul.cu -lcublas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIQVbIu7v2Dm",
        "outputId": "eccdeecc-14e4-413e-f439-d60eb2b4827c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quelques éléments de la matrice résultante C:\n",
            "2487438.00 2368418.00 2519780.00 2428181.00 2591257.00 \n",
            "2543611.00 2506439.00 2522125.00 2509590.00 2713108.00 \n",
            "2528800.00 2443670.00 2537227.00 2477653.00 2624294.00 \n",
            "2523920.00 2420775.00 2488090.00 2418354.00 2610738.00 \n",
            "2456321.00 2315729.00 2422694.00 2397392.00 2530055.00 \n"
          ]
        }
      ],
      "source": [
        "!./large_matrix_mul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcDNG1HdzvDa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovWbET8k5Cgj"
      },
      "source": [
        "\n",
        "N'hésitez pas à jeter un œil à la doc de cuBLAS ! Un vrai IT geek sait toujours où chercher les bonnes infos.\n",
        "\n",
        "https://docs.nvidia.com/cuda/pdf/CUBLAS_Library.pdf\n",
        "\n",
        "\n",
        "## 1. Analyse de la structure du code\n",
        "- **a)** Décrivez les principales étapes du programme (allocation mémoire, transfert de données, calcul, etc.). Pourquoi est-il nécessaire de gérer explicitement la mémoire entre le CPU et le GPU ?\n",
        "- **b)** Quel est le rôle du `cublasHandle_t` dans ce programme ? Pourquoi doit-on appeler `cublasCreate` et `cublasDestroy` ?\n",
        "- **c)** Expliquez la signification des paramètres `CUBLAS_OP_N` dans l’appel à `cublasSgemm`. Que se passerait-il si l’un d’eux était remplacé par `CUBLAS_OP_T` (transposition) ?\n",
        "- **d)** Quel est l’impact de l’ordre de stockage des matrices en mémoire (row-major vs column-major) sur l'utilisation de cuBLAS ?\n",
        "\n",
        "## 2. Comparaison cuBLAS vs calcul natif\n",
        "- **a)** Implémentez une version \"native\" de la multiplication de matrices $ C = A \\cdot B $ en C sur le CPU (boucles classiques). Comparez qualitativement la complexité algorithmique de cette approche avec celle de `cublasSgemm`.\n",
        "- **b)** Modifiez le programme pour mesurer le temps d’exécution de la multiplication avec `cublasSgemm` (utilisez `cudaEvent_t`). Implémentez ensuite votre version native sur CPU et comparez les performances pour $ N = 1024 $. Quelles différences observez-vous, et pourquoi ?\n",
        "- **c)** Si vous deviez écrire une version parallèle \"native\" sur GPU (sans cuBLAS, avec un kernel CUDA), quelles seraient les principales difficultés à surmonter par rapport à l’utilisation de cuBLAS ?\n",
        "- **d)** Est-il pertinent d’utiliser cuBLAS pour des matrices très petites (ex. $ N = 16 $) ? Justifiez votre réponse en considérant les surcharges (overheads) liées au GPU.\n",
        "\n",
        "## 3. Gestion de la mémoire\n",
        "- **a)** Pourquoi utilise-t-on `cudaMalloc` et `cudaMemcpy` au lieu de travailler directement avec les matrices $ A $, $ B $, et $ C $ sur le GPU ? Que se passerait-il si on omettait un appel à `cudaMemcpy` avant `cublasSgemm` ?\n",
        "- **b)** Calculez la quantité totale de mémoire GPU allouée dans ce programme pour $ N = 1024 $ (en Mo). Comment cette consommation évoluerait-elle si $ N $ doublait ?\n",
        "- **c)** Que risquerait-on en oubliant d’appeler `cudaFree` pour $ d_A $, $ d_B $, et $ d_C $ ? Comment vérifier si la mémoire GPU est bien libérée ?\n",
        "- **d)** Quelle est la différence entre `cudaMallocManaged` et `cudaMalloc` ? Dans quel cas pourrait-on utiliser `cudaMallocManaged` pour simplifier le transfert de données entre CPU et GPU ?\n",
        "\n",
        "## 4. Paramètres de cuBLAS\n",
        "- **a)** Expliquez le rôle des paramètres `alpha` et `beta` dans `cublasSgemm`. Que se passerait-il si `beta = 1.0f` au lieu de `0.0f`, sans initialiser $ d_C $ au préalable ?\n",
        "- **b)** Modifiez le code pour calculer $ C = 2A \\cdot B + 3C $ au lieu de $ C = A \\cdot B $. Quels paramètres de `cublasSgemm` faudrait-il ajuster ?\n",
        "- **c)** Comment adapteriez-vous le code pour multiplier des matrices non carrées, par exemple $ A $ de taille $512 \\times 1024$ et $ B $ de taille $ 1024 \\times 768 $ ?\n",
        "- **d)** Existe-t-il d’autres fonctions dans cuBLAS pour la multiplication de matrices en mode batched (plusieurs multiplications en parallèle) ? Comment les utiliser ?\n",
        "\n",
        "## 5. Optimisation et performance\n",
        "- **a)** Pourquoi cuBLAS est-il généralement plus rapide qu’une implémentation CPU native pour de grandes matrices comme $N = 1024$ ? Quels aspects du GPU exploite-t-il ?\n",
        "- **b)** Si $ N $ était beaucoup plus petit (ex. $ N = 16 $), pensez-vous que cuBLAS resterait avantageux par rapport à une version CPU ? Justifiez votre réponse en considérant les overheads (surcharges) liés au GPU.\n",
        "- **c)** Proposez une modification du code pour traiter des matrices trop grandes pour tenir entièrement dans la mémoire GPU (par exemple, $ N = 10000 $). Comment découperiez-vous le problème ?\n",
        "- **d)** Comment le choix des blocs et des threads impacte-t-il la performance si on implémente `Sgemm` en kernel CUDA personnalisé ?\n",
        "\n",
        "## 6. Extensions et réflexion\n",
        "- **a)** Adaptez le code pour utiliser des nombres en double précision (`double`) avec `cublasDgemm`. Quels changements sont nécessaires dans les types de données et les appels de fonctions ?\n",
        "- **b)** Imaginez que vous devez multiplier plusieurs paires de matrices consécutivement (ex. $ C = A \\cdot B $, puis $ D = C \\cdot E $). Comment optimiseriez-vous le code pour minimiser les transferts de données entre CPU et GPU ?\n",
        "- **c)** Si vous aviez accès à plusieurs GPU, comment modifieriez-vous le code pour distribuer le calcul avec cuBLAS ? Quels défis cela poserait-il ?\n",
        "- **d)** cuBLAS offre-t-il des optimisations pour les architectures GPU récentes comme Ampere ou Hopper ? Quelles fonctionnalités avancées pourrait-on exploiter ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Analyse de la structure du code\n",
        "\n",
        "**a) Étapes principales et gestion de la mémoire**\n",
        "\n",
        "- **Allocation et initialisation** : Le programme alloue la mémoire sur le CPU pour les matrices A, B et C, et initialise A et B avec des valeurs aléatoires.\n",
        "- **Création du contexte cuBLAS** : Un handle (`cublasHandle_t`) est créé avec `cublasCreate` pour gérer l'état de la bibliothèque cuBLAS.\n",
        "- **Allocation GPU et transfert** : La mémoire est allouée sur le GPU pour les copies de A, B et C à l'aide de `cudaMalloc`. Les données de A et B sont transférées du CPU vers le GPU via `cudaMemcpy`.\n",
        "- **Calcul sur GPU** : La multiplication matricielle est effectuée sur le GPU en appelant `cublasSgemm`.\n",
        "- **Retour des résultats** : La matrice résultat C est copiée du GPU vers le CPU.\n",
        "- **Libération des ressources** : Les mémoires allouées sur le GPU et le CPU sont libérées, et le handle cuBLAS est détruit avec `cublasDestroy`.\n",
        "\n",
        "La gestion explicite de la mémoire entre le CPU et le GPU est nécessaire car ces deux entités disposent de mémoires physiques séparées. Les données doivent être transférées explicitement pour être accessibles par le GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**b) Rôle du `cublasHandle_t`**\n",
        "\n",
        "Le `cublasHandle_t` est un objet qui représente le contexte d'exécution de la bibliothèque cuBLAS. Il permet de conserver l'état et les configurations nécessaires pour exécuter les fonctions de la bibliothèque. `cublasCreate` initialise ce contexte et alloue les ressources nécessaires, tandis que `cublasDestroy` libère ces ressources une fois les opérations terminées."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**c) Signification des paramètres `CUBLAS_OP_N`**\n",
        "\n",
        "Les paramètres `CUBLAS_OP_N` indiquent que les matrices ne doivent pas être transposées lors de l'appel à `cublasSgemm`. Si l’un d’eux était remplacé par `CUBLAS_OP_T`, la matrice correspondante serait transposée avant la multiplication, ce qui modifierait l'ordre des éléments et pourrait changer les dimensions ou les valeurs du résultat final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**d) Impact de l’ordre de stockage (row-major vs column-major)**\n",
        "\n",
        "CuBLAS utilise par défaut le format column-major (comme en Fortran), alors que le C standard emploie souvent le format row-major. Ce décalage peut entraîner des interprétations erronées des matrices lors du calcul. Il faut alors soit adapter les paramètres de la fonction (par exemple, en transposant les matrices logiquement), soit convertir explicitement les données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Comparaison cuBLAS vs calcul natif\n",
        "\n",
        "**a) Implémentation native vs cuBLAS**\n",
        "\n",
        "Une multiplication matricielle native en C impliquerait trois boucles imbriquées, ayant une complexité de O(N³). Bien que la complexité asymptotique soit identique à celle de `cublasSgemm`, ce dernier est hautement optimisé et exploite le parallélisme massif du GPU, ce qui permet d'obtenir des performances bien supérieures pour de grandes matrices.\n",
        "\n",
        "**b) Mesure du temps d’exécution et comparaison des performances**\n",
        "\n",
        "En utilisant `cudaEvent_t` pour mesurer le temps d'exécution, on constate généralement que l'exécution avec `cublasSgemm` est beaucoup plus rapide pour N = 1024 par rapport à une implémentation CPU native. Le GPU, grâce à son architecture parallèle, permet de traiter simultanément un grand nombre d'opérations, contrairement au CPU dont les ressources parallèles sont limitées.\n",
        "\n",
        "**c) Difficultés d'une version parallèle native sur GPU**\n",
        "\n",
        "Écrire une version native en CUDA impliquerait de gérer manuellement :\n",
        "- L'organisation des threads et des blocs pour assurer une utilisation efficace des cœurs GPU.\n",
        "- La gestion de la mémoire partagée pour optimiser les accès aux données.\n",
        "- La synchronisation entre threads et la minimisation des accès non coalescés.\n",
        "\n",
        "Ces aspects sont abstraits par cuBLAS, qui offre une implémentation hautement optimisée sans que l'utilisateur ait à gérer ces détails complexes.\n",
        "\n",
        "**d) Pertinence de cuBLAS pour de petites matrices**\n",
        "\n",
        "Pour des matrices de petite taille (ex. N = 16), les surcharges liées au lancement de kernels et aux transferts de données entre le CPU et le GPU peuvent être significatives, au point que l'avantage de l'accélération par GPU est perdu. Dans ce cas, une implémentation native sur CPU peut être plus efficace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Gestion de la mémoire\n",
        "\n",
        "**a) Rôle de `cudaMalloc` et `cudaMemcpy`**\n",
        "\n",
        "Les fonctions `cudaMalloc` et `cudaMemcpy` sont utilisées pour allouer et transférer explicitement les données dans l'espace mémoire du GPU, qui est distinct de celui du CPU. Si on omettait l'appel à `cudaMemcpy` avant `cublasSgemm`, le GPU ne disposerait pas des données initialisées sur le CPU, ce qui conduirait à des erreurs ou à des résultats incorrects.\n",
        "\n",
        "**b) Calcul de la mémoire GPU allouée pour N = 1024**\n",
        "\n",
        "Pour chaque matrice :\n",
        "- Taille = 1024 x 1024 x 4 bytes (pour un float) ≈ 4 Mo.\n",
        "\n",
        "Pour 3 matrices (A, B, C) : environ 12 Mo au total.\n",
        "\n",
        "Si N doublait (N = 2048), la mémoire par matrice serait proportionnelle à N², donc environ 4 fois plus élevée, soit environ 48 Mo pour les trois matrices.\n",
        "\n",
        "**c) Risques en oubliant `cudaFree`**\n",
        "\n",
        "Omettre d'appeler `cudaFree` pour libérer `d_A`, `d_B` et `d_C` provoquerait une fuite de mémoire sur le GPU, réduisant ainsi la mémoire disponible pour d'autres calculs. On peut vérifier la libération de la mémoire en utilisant des outils comme `nvidia-smi`, qui affichent l'utilisation de la mémoire GPU.\n",
        "\n",
        "**d) Différence entre `cudaMallocManaged` et `cudaMalloc`**\n",
        "\n",
        "`cudaMallocManaged` alloue une mémoire unifiée accessible à la fois par le CPU et le GPU, simplifiant ainsi le transfert de données. En revanche, `cudaMalloc` réserve de la mémoire exclusivement sur le GPU, nécessitant des transferts explicites via `cudaMemcpy`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Paramètres de cuBLAS\n",
        "\n",
        "**a) Rôle des paramètres `alpha` et `beta`**\n",
        "\n",
        "Dans l'appel à `cublasSgemm`, le calcul réalisé est :\n",
        "\n",
        "    C = alpha * A * B + beta * C\n",
        "\n",
        "Les paramètres `alpha` et `beta` permettent de scaler respectivement le produit matriciel et la matrice C déjà existante. Si `beta` était fixé à 1.0f sans initialiser C, les valeurs non définies de C seraient ajoutées au résultat, entraînant des erreurs.\n",
        "\n",
        "**b) Modification pour calculer C = 2A * B + 3C**\n",
        "\n",
        "Il suffit de remplacer `alpha` par 2.0f et `beta` par 3.0f dans l'appel à `cublasSgemm`.\n",
        "\n",
        "**c) Adaptation pour des matrices non carrées**\n",
        "\n",
        "Pour multiplier, par exemple, une matrice A de taille 512x1024 par une matrice B de taille 1024x768, il faut adapter les dimensions dans l'appel à `cublasSgemm` :\n",
        "- m = 512 (nombre de lignes de A),\n",
        "- n = 768 (nombre de colonnes de B),\n",
        "- k = 1024 (nombre de colonnes de A ou lignes de B).\n",
        "\n",
        "Les paramètres de stride et les leading dimensions doivent également être ajustés en conséquence.\n",
        "\n",
        "**d) Fonctions batched dans cuBLAS**\n",
        "\n",
        "CuBLAS propose des fonctions pour la multiplication de matrices en mode batched, telles que `cublasSgemmBatched` et `cublasSgemmStridedBatched`, permettant d'exécuter plusieurs multiplications en parallèle en passant des tableaux de pointeurs vers les matrices ou en utilisant un stride constant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Optimisation et performance\n",
        "\n",
        "**a) Pourquoi cuBLAS est-il plus rapide pour N = 1024 ?**\n",
        "\n",
        "CuBLAS exploite le parallélisme massif des GPU, utilisant des milliers de cœurs et des optimisations spécifiques (comme la gestion efficace des accès mémoire et l'utilisation de Tensor Cores sur certaines architectures) pour accélérer les calculs de grande envergure.\n",
        "\n",
        "**b) Avantage de cuBLAS pour de petites matrices (ex. N = 16)**\n",
        "\n",
        "Pour des matrices de très petite taille, l'overhead associé aux lancements de kernels et aux transferts de données entre CPU et GPU peut surpasser les bénéfices du calcul parallèle. Ainsi, pour N = 16, une implémentation CPU native pourrait être plus performante en raison de la faible charge de calcul et de l'absence d'overhead de transfert.\n",
        "\n",
        "**c) Traitement de matrices trop grandes pour la mémoire GPU**\n",
        "\n",
        "Pour des matrices trop grandes (par exemple, N = 10000), il est pertinent de découper le problème en sous-blocs (technique de tiling). Chaque bloc est multiplié séparément sur le GPU et les résultats sont ensuite assemblés pour former la matrice finale. Cette approche permet de traiter des matrices dont la taille excède la mémoire disponible sur le GPU.\n",
        "\n",
        "**d) Impact du choix des blocs et des threads dans un kernel CUDA personnalisé**\n",
        "\n",
        "Le choix de la taille des blocs et du nombre de threads influe directement sur l'efficacité de l'utilisation des ressources GPU, la coalescence des accès mémoire et l'utilisation de la mémoire partagée. Une mauvaise configuration peut entraîner une sous-utilisation des capacités du GPU et une dégradation significative des performances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Extensions et réflexion\n",
        "\n",
        "**a) Passage en double précision**\n",
        "\n",
        "Pour utiliser des nombres en double précision, il faut :\n",
        "- Remplacer le type `float` par `double` dans la déclaration des matrices.\n",
        "- Utiliser `cublasDgemm` au lieu de `cublasSgemm`.\n",
        "- Adapter les constantes `alpha` et `beta` (par exemple, 1.0f deviendra 1.0 en double).\n",
        "\n",
        "**b) Optimisation pour des multiplications consécutives**\n",
        "\n",
        "Si plusieurs multiplications doivent être réalisées (par exemple, C = A * B suivi de D = C * E), il est avantageux de conserver les matrices intermédiaires dans la mémoire GPU afin de minimiser les transferts entre le CPU et le GPU.\n",
        "\n",
        "**c) Utilisation de plusieurs GPU**\n",
        "\n",
        "Pour exploiter plusieurs GPU, il est nécessaire de partitionner les matrices et de distribuer les calculs entre les différents dispositifs. Les principaux défis incluent la synchronisation inter-GPU, la gestion des transferts de données entre GPU, et la répartition équilibrée de la charge de travail.\n",
        "\n",
        "**d) Optimisations sur les architectures récentes**\n",
        "\n",
        "CuBLAS intègre des optimisations pour les architectures récentes comme Ampere ou Hopper, notamment via l'utilisation des Tensor Cores et des fonctionnalités avancées offertes par cuBLASLt, qui permettent un contrôle plus fin et des performances accrues pour certaines opérations de multiplication matricielle."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
